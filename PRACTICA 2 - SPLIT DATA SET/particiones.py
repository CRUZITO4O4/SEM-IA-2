# -*- coding: utf-8 -*-
"""particiones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HemBzrvsu_nkRmtqQYu2-VNSMyylzq_n
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, KFold
from mpl_toolkits.mplot3d import Axes3D

def partition_random(train_percentage, test_percentage):
    # Calcular los tamaños exactos de los conjuntos de entrenamiento y prueba
    train_size = int(len(data) * train_percentage)
    test_size = int(len(data) * test_percentage)

    # Obtener índices únicos para los conjuntos de entrenamiento y prueba
    indices = np.random.permutation(len(data))
    train_indices = indices[:train_size]
    test_indices = indices[train_size:train_size + test_size]

    # Obtener los datos de entrenamiento y prueba según los índices
    train_data = data.iloc[train_indices]
    test_data = data.iloc[test_indices]

    # Imprimir información sobre la partición
    # print(f'Partición random: {len(train_data)} datos de entrenamiento, {len(test_data)} datos de prueba')

    # Guardar los conjuntos de entrenamiento y prueba en archivos separados
    train_data.to_csv(f'train_partition_random.csv', index=False)
    test_data.to_csv(f'test_partition_random.csv', index=False)

def partition_in_order(train_percentage, test_percentage):
    # Calcular los tamaños exactos de los conjuntos de entrenamiento y prueba
    train_size = int(len(data) * train_percentage)
    test_size = int(len(data) * test_percentage)

    # Seleccionar los primeros 'train_size' datos para entrenamiento
    train_data = data.iloc[:train_size]

    # Seleccionar los siguientes 'test_size' datos para prueba
    test_data = data.iloc[train_size:train_size + test_size]

    # Imprimir información sobre la partición
    # print(f'Partición en orden: {len(train_data)} datos de entrenamiento, {len(test_data)} datos de prueba')

    # Guardar los conjuntos de entrenamiento y prueba en archivos separados
    train_data.to_csv(f'train_partition_in_order.csv', index=False)
    test_data.to_csv(f'test_partition_in_order.csv', index=False)

def partition_in_order_test_first(train_percentage, test_percentage):
    # Calcular los tamaños exactos de los conjuntos de entrenamiento y prueba
    train_size = int(len(data) * train_percentage)
    test_size = int(len(data) * test_percentage)

    # Seleccionar los primeros 'test_size' datos para prueba
    test_data = data.iloc[:test_size]

    # Seleccionar los datos restantes para entrenamiento
    train_data = data.iloc[test_size:test_size + train_size]

    # Imprimir información sobre la partición
    # print(f'Partición en orden (test primero): {len(train_data)} datos de entrenamiento, {len(test_data)} datos de prueba')

    # Guardar los conjuntos de entrenamiento y prueba en archivos separados
    train_data.to_csv(f'train_partition_in_order_test_first.csv', index=False)
    test_data.to_csv(f'test_partition_in_order_test_first.csv', index=False)

def partition_alternating_blocks_10(train_percentage, test_percentage):
    # Calcular los tamaños exactos de los conjuntos de entrenamiento y prueba
    block_size = 10
    train_size = int(len(data) * train_percentage)
    test_size = int(len(data) * test_percentage)

    # Inicializar listas para almacenar los conjuntos de entrenamiento y prueba
    train_data_list = []
    test_data_list = []

    # Particionar los datos en bloques alternando desde el principio y el final
    for i in range(0, train_size, block_size):
        train_block = data.iloc[i:i + block_size]
        train_data_list.append(train_block)

    for i in range(len(data) - test_size, len(data), block_size):
        test_block = data.iloc[i:i + block_size]
        test_data_list.append(test_block)

    # Concatenar los bloques para obtener los conjuntos finales de entrenamiento y prueba
    train_data = pd.concat(train_data_list)
    test_data = pd.concat(test_data_list)

    # Imprimir información sobre la partición
    # print(f'Partición alternando bloques: {len(train_data)} datos de entrenamiento, {len(test_data)} datos de prueba')

    # Guardar los conjuntos de entrenamiento y prueba en archivos separados
    train_data.to_csv(f'train_partition_alternating_blocks_10.csv', index=False)
    test_data.to_csv(f'test_partition_alternating_blocks_10.csv', index=False)

def partition_alternating_blocks_20(train_percentage, test_percentage):
    # Calcular los tamaños exactos de los conjuntos de entrenamiento y prueba
    block_size = 20
    train_size = int(len(data) * train_percentage)
    test_size = int(len(data) * test_percentage)

    # Inicializar listas para almacenar los conjuntos de entrenamiento y prueba
    train_data_list = []
    test_data_list = []

    # Particionar los datos en bloques alternando desde el principio y el final
    for i in range(0, train_size, block_size):
        train_block = data.iloc[i:i + block_size]
        train_data_list.append(train_block)

    for i in range(len(data) - test_size, len(data), block_size):
        test_block = data.iloc[i:i + block_size]
        test_data_list.append(test_block)

    # Concatenar los bloques para obtener los conjuntos finales de entrenamiento y prueba
    train_data = pd.concat(train_data_list)
    test_data = pd.concat(test_data_list)

    # Imprimir información sobre la partición
    # print(f'Partición alternando bloques: {len(train_data)} datos de entrenamiento, {len(test_data)} datos de prueba')

    # Guardar los conjuntos de entrenamiento y prueba en archivos separados
    train_data.to_csv(f'train_partition_alternating_blocks_20.csv', index=False)
    test_data.to_csv(f'test_partition_alternating_blocks_20.csv', index=False)

# PERCEPTRON
def perceptron_activation(summation):
    return 1 if summation >= 0 else 0

def perceptron(inputs, weights, bias):
    summation = np.dot(inputs, weights) + bias
    return perceptron_activation(summation)

# LEER CSV(EXCEL CON LOS DATOS)
def read_data(file):
    data = np.genfromtxt(file, delimiter=',')
    inputs = data[:, :-1]
    outputs = data[:, -1]
    return inputs, outputs

# ENTRENAMIENTO DEL PERCEPTRON
def train_perceptron(inputs, outputs, learning_rate, max_epochs, convergence_criterion):
    num_inputs = inputs.shape[1]
    num_patterns = inputs.shape[0]

    weights = np.random.rand(num_inputs)
    bias = np.random.rand()
    epochs = 0
    convergence = False

    while epochs < max_epochs and not convergence:
        convergence = True
        for i in range(num_patterns):
            input_pattern = inputs[i]
            output_prediction = outputs[i]
            output_received = np.dot(weights, input_pattern) + bias
            error = output_prediction - output_received

            if abs(error) > convergence_criterion:
                convergence = False
                weights += learning_rate * error * input_pattern
                bias += learning_rate * error
        epochs += 1
    return weights, bias

# TEST
def test_perceptron(inputs, weights, bias):
    output_received = np.dot(inputs, weights) + bias
    return np.vectorize(perceptron_activation)(output_received)

def plot_3d_dataset(inputs, outputs, title="Dataset"):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    unique_classes = np.unique(outputs)
    colors = plt.cm.Paired(np.linspace(0, 1, len(unique_classes)))

    for i, class_label in enumerate(unique_classes):
        class_indices = np.where(outputs == class_label)
        ax.scatter(inputs[class_indices, 0], inputs[class_indices, 1], inputs[class_indices, 2], label=f'Class {int(class_label)}', c=[colors[i]])

    ax.set_title(title)
    ax.set_xlabel("Feature 1")
    ax.set_ylabel("Feature 2")
    ax.set_zlabel("Feature 3")
    ax.legend()
    plt.show()

if __name__ == "__main__":

    datasets = ['spheres2d10.csv', 'spheres2d50.csv', 'spheres2d70.csv']
    # SELECCIONAR DATASET
    print("Seleccione el conjunto de datos:")
    print("1. spheres2d10.csv")
    print("2. spheres2d50.csv")
    print("3. spheres2d70.csv")

    dataset_choice = int(input("Ingrese el número correspondiente al conjunto de datos deseado: "))

    if dataset_choice not in [1, 2, 3]:
        print("Opción no válida. Saliendo del programa.")
        exit()

    selected_dataset = datasets[dataset_choice - 1]

    # SELECCIONAR METODO DE PARTICIÓN DEL DATASET
    print("Seleccione el tipo de partición:")
    print("1. Partición Random")
    print("2. Partición en Orden")
    print("3. Partición en Orden (Test Primero)")
    print("4. Partición Alternando Bloques de 10")
    print("5. Partición Alternando Bloques de 20")

    partition_type = int(input("Ingrese el número correspondiente al tipo de partición deseado: "))

    partition_names = {
        1: "Partición Random",
        2: "Partición en Orden",
        3: "Partición en Orden (Test Primero)",
        4: "Partición Alternando Bloques de 10",
        5: "Partición Alternando Bloques de 20"
    }

    # SELECCIONAR LOS PORCENTAJES
    train_percentage = float(input("Ingrese el porcentaje de datos para entrenamiento (ej. 0.8): "))
    test_percentage = float(input("Ingrese el porcentaje de datos para prueba (ej. 0.2): "))

    if partition_type in partition_names:
        partition_name = partition_names[partition_type]
        print(f"Seleccionaste: {partition_name}")

        if partition_type == 1:
            partition_random(train_percentage, test_percentage)
            training_file = 'train_partition_random.csv'
            test_file = 'test_partition_rangom.csv'
        elif partition_type == 2:
            partition_in_order(train_percentage,test_percentage)
            training_file = 'train_partition_in_order.csv'
            test_file = 'test_partition_in_order.csv'
        elif partition_type == 3:
            partition_in_order_test_first(train_percentage,test_percentage)
            training_file = 'train_partition_in_order_test_first.csv'
            test_file = 'test_partition_in_order_test_first.csv'
        elif partition_type == 4:
            partition_alternating_blocks_10(train_percentage,test_percentage)
            training_file = 'train_partition_alternating_blocks_10.csv'
            test_file = 'test_partition_alternating_blocks_10.csv'
        elif partition_type == 5:
            partition_alternating_blocks_20(train_percentage,test_percentage)
            training_file = 'train_partition_alternating_blocks_20.csv'
            test_file = 'test_partition_alternating_blocks_20.csv'
    else:
        print("Opción no válida. Saliendo del programa.")
        exit()

    inputs_train, outputs_train = read_data(training_file)
    inputs_test, outputs_test = read_data(test_file)

    # PARAMETROS
    max_epochs = 30
    learning_rate = 0.1
    convergence_criterion = 0.01  # ALTERACIONES ALEATORIAS 5%

    # ENTRENAMIENTO
    trained_weights, trained_bias = train_perceptron(inputs_train, outputs_train, learning_rate,
    max_epochs, convergence_criterion)
    print("Perceptrón entrenado con éxito.")

    # PERCEPTRON CON DATOS DE ENTRADA
    outputs_predictions = test_perceptron(inputs_test, trained_weights, trained_bias)

    # SACAR METRICAS
    accuracy = accuracy_score(outputs_test, outputs_predictions)
    precision = precision_score(outputs_test, outputs_predictions, average='weighted')
    recall = recall_score(outputs_test, outputs_predictions, average='weighted', zero_division=1)
    f1 = f1_score(outputs_test, outputs_predictions, average='weighted')

    print("Accuracy del perceptrón en datos de prueba (Accuracy):", accuracy)
    print("Precisión del perceptrón en datos de prueba (Precision):", precision)
    print("Recall del perceptrón en datos de prueba (Recall):", recall)
    print("F1 del perceptrón en datos de prueba (F1):", f1)

    plot_3d_dataset(inputs_test, outputs_predictions, title=f"Conjunto de Datos {selected_dataset} - Técnica: {partition_name}")

